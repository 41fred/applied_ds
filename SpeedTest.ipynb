{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed test example\n",
    "\n",
    "I have a large number of rows in my data (about 750k!), and a lot of the data isn't usable in it's current format.\n",
    "\n",
    "When I started to try and clean it, I ran into a lot of problems with processing speed. Through some trial and error, I found a way to make my function run almost 9000 times faster!\n",
    "\n",
    "## The original function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sixteenData = pd.read_csv('2016Short.csv') #Read the file into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {\"urls\":{\"web\":{\"discover\":\"http://www.kicksta...\n",
       "1    {\"urls\":{\"web\":{\"discover\":\"http://www.kicksta...\n",
       "2    {\"urls\":{\"web\":{\"discover\":\"http://www.kicksta...\n",
       "3    {\"urls\":{\"web\":{\"discover\":\"http://www.kicksta...\n",
       "4    {\"urls\":{\"web\":{\"discover\":\"http://www.kicksta...\n",
       "Name: category, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sixteenData.category.head() #Show the head of the column I want to change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in this column was originally in JSON format. When it was imported to a csv and then read into Pandas, it lost it's JSON formatting and such, I can't use it as a JSON object anymore. So I'm going to try and extract the text I need using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catStrip2016(target):\n",
    "    '''\n",
    "     This function strips out the parent category and subcategory of the project\n",
    "    The regex.split method with r'\\W+' splits the input string at all non-alphanumeric characters\n",
    "    The list locations where the parent/subcategory are change between scrape dates\n",
    "    So the correct positions have to be chosen for each\n",
    "    '''\n",
    "    x = re.split(r'\\W+', target) #Split the input at all non-alphanumerics\n",
    "    results = [x[10], x[11]] #Extract the two parts I'm interested in\n",
    "    return(results) #Return the results of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrismay/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/chrismay/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Last operation took  0.14040517807006836  seconds\n",
      "1 Last operation took  0.15074706077575684  seconds\n",
      "2 Last operation took  0.14543581008911133  seconds\n",
      "3 Last operation took  0.1445779800415039  seconds\n",
      "4 Last operation took  0.1438159942626953  seconds\n"
     ]
    }
   ],
   "source": [
    "timeCounter4 = []\n",
    "for entry in range(5):\n",
    "    start = time.time()\n",
    "    temp = sixteenData.category[entry]\n",
    "    categories = catStrip2016(temp)\n",
    "    sixteenData.iloc[entry]['Parent'] = temp[0]\n",
    "    sixteenData.iloc[entry]['Subcategory'] = temp[1]\n",
    "    end = time.time()\n",
    "    timeCounter4.append(end - start)\n",
    "    print(entry, 'Last operation took ',end - start,' seconds')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from some warnings, the function executed okay. The problem here is each iteration took 0.15 seconds. Stretched out over the entire length of the dataframe (abut 189,000 rows), and this operation would take around 8 hours to complete! \n",
    "\n",
    "I have four .csv files that I need to clean, and I have four columns in each .csv file that need similar operations done. If I used this method, the whole thing would take 128 hours to run! \n",
    "\n",
    "Clearly I need to find another way to work. My first guess is that either the .iloc function adds to the complexity since it has to search for the right location on every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second try - use `.append()` to make a separate data frame and then merge it\n",
    "\n",
    "I want to try and break up the operations, so for this try I'm going to pull the entry out of the \n",
    "sixteenData dataframe, run the function on it, and then append it to the end of a \n",
    "new dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Last operation took  0.0035181045532226562  seconds\n",
      "1 Last operation took  0.0032079219818115234  seconds\n",
      "2 Last operation took  0.0033278465270996094  seconds\n",
      "3 Last operation took  0.004068851470947266  seconds\n",
      "4 Last operation took  0.003242969512939453  seconds\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This time, keep the same function but append the results to the end of a\n",
    "new dataframe rather than locating it in the original\n",
    "\n",
    "This block of code extracts the desired entry in the original dataframe,\n",
    "runs the function on it, then appends it to the end of a new dataframe\n",
    "'''\n",
    "\n",
    "tempFrame = pd.DataFrame()\n",
    "for entry in range(5):\n",
    "    start = time.time()\n",
    "    temp = catStrip2016(sixteenData.category[entry])\n",
    "    tempFrame.append({'Parent':temp[0],\n",
    "                      'Subcategory':temp[1]\n",
    "                      } , ignore_index = True)\n",
    "    end = time.time()\n",
    "    print(entry, 'Last operation took ',end - start,' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great start! We've drastically reduced the time it takes for each iteration. I know from personal experience that inserting the new dataframe into the original won't take long, so this is a major improvement. \n",
    "\n",
    "It's still pretty slow though. Running this method on all the rows will take around 11 minutes (0.0035 * 189000). For my entire dataset, I'm looking at about 3 hours of computation time, and I can't do that. \n",
    "\n",
    "At this point, I figured the size of the dataframe I was working with might be slowing it down for some reason (the original file is about 500 MB).\n",
    "\n",
    "So I'm going to try and isolate the data of interest a little more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third try - Use `pd.pop()` to isolate the column I want to work with\n",
    "\n",
    "For this try, I used `pd.pop()` to isolate the column of interest. Then I ran the code from try 2 to see if there's any improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Last operation took  0.016701936721801758  seconds\n",
      "1 Last operation took  0.0019998550415039062  seconds\n",
      "2 Last operation took  0.0019178390502929688  seconds\n",
      "3 Last operation took  0.0019621849060058594  seconds\n",
      "4 Last operation took  0.0023949146270751953  seconds\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This block of codes pops a specific column from a pandas dataframe,\n",
    "runs the cleaning operation on just that column, and then appends the\n",
    "result to a new dataframe\n",
    "'''\n",
    "tempFrame2 = pd.DataFrame() #Create a blank dataframe for later use\n",
    "dummy = sixteenData.copy() #Make a copy of the original dataframe to keep it intact \n",
    "loneColumn = dummy.pop('category') #pop the category column into a separate dataframe\n",
    "for entry in range(5):\n",
    "    start = time.time()\n",
    "    temp = catStrip2016(loneColumn[entry])\n",
    "    tempFrame.append({'Categories':temp}, ignore_index = True )\n",
    "    end = time.time()\n",
    "    print(entry, 'Last operation took ',end - start,' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that was ...underwhelming. There was a 2x increase in speed, but I'm still looking at a few hours in total to run this on my whole dataset.\n",
    "\n",
    "It's time to get serious, I'm asking the internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desperation - Turning to Google\n",
    "\n",
    "At this point, I've exhausted my knowledge of Python. From my limited understanding of computer science, I know that `for` loops can get computationally expensive for a large sample size. \n",
    "\n",
    "For my last attempt, I just ran a google search for methods that apply a function to an entire column in Pandas.\n",
    "\n",
    "To my surprise, Pandas has a method called .... `pd.apply()` \n",
    "\n",
    "(I feel like the Python developers are taking a cheap shot at newbies with this one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Start\n",
      "Operation took  3.229017734527588  seconds\n"
     ]
    }
   ],
   "source": [
    "print('Function Start')\n",
    "start = time.time()\n",
    "sixteenData['category_list'] = sixteenData['category'].apply(catStrip2016)\n",
    "end = time.time()\n",
    "total = end - start\n",
    "print('Operation took ', total, ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!\n",
    "\n",
    "Using the `.apply()` method, running this function on a single column over all of the rows took only 3.23 seconds!\n",
    "\n",
    "To compare properly, that means this operation takes 3.23/189000 seconds per entry, or 0.0000171 seconds.\n",
    "\n",
    "That's an improvement of 8,500x the original (badly coded) operation. Now I have a time frame that I can work with, and make a proper cleaning script for each of my .csv files \n",
    "\n",
    "From what I understand, programmers use \"big 0 notation\" (ie O(1), O(N), O(n<sup>2</sup>)) to keep track of algorithm complexity and how fast things get calculated. \n",
    "\n",
    "Here's a quick primer on big O: [link](https://rob-bell.net/2009/06/a-beginners-guide-to-big-o-notation/)\n",
    "\n",
    "If I was a better student, I'd estimate the O() complexity for each of the above cases. Maybe next time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
